= Chat Models: Mistral AI

Text generation with LLMs via link:https://mistral.ai/[Mistral AI].

:model-provider: Mistral AI
include::/docs/models/chat/api-description.adoc[]

include::/docs/models/providers/mistral-ai.adoc[]

== Running the application

Run the Spring Boot application.

[source,shell]
----
./gradlew bootRun
----

== Calling the application

You can now call the application that will use Mistral AI to generate text based on a default prompt.
This example uses https://httpie.io[httpie] to send HTTP requests.

[source,shell]
----
http :8080/chat -b
----

Try passing your custom prompt and check the result.

[source,shell]
----
http :8080/chat question=="What is the capital of Italy?" -b
----

The next request is configured with a custom temperature value to obtain a more creative, yet less precise answer.

[source,shell]
----
http :8080/chat/generic-options question=="Why is a raven like a writing desk? Give a short answer." -b
----

The next request is configured with Mistral AI-specific customizations.

[source,shell]
----
http :8080/chat/provider-options question=="What can you see beyond what you can see? Give a short answer." -b
----

The final request returns the model's answer as a stream.

[source,shell]
----
http --stream :8080/chat/stream question=="Why is a raven like a writing desk? Answer in 3 paragraphs." -b
----
